{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сходство картинки с текстом\n",
    "\n",
    "# 0.49416\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms, models\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------------------\n",
    "# Конфигурация путей\n",
    "# ----------------------------\n",
    "TRAIN_TSV = \"train_df.tsv\"\n",
    "TEST_TSV = \"test_df.tsv\"\n",
    "TRAIN_IMG_DIR = \"train\"\n",
    "TEST_IMG_DIR = \"test\"\n",
    "OUTPUT_FILE = \"submissionx2.tsv\"\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Загрузка данных\n",
    "# ----------------------------\n",
    "train_df = pd.read_csv(TRAIN_TSV, sep='\\t')\n",
    "test_df = pd.read_csv(TEST_TSV, sep='\\t')\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Предобработка изображений\n",
    "# ----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Загрузка предобученной ResNet (без головы)\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*list(resnet.children())[:-1])  # Убираем последний слой\n",
    "resnet.eval()\n",
    "resnet.to(device)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def extract_img_features(img_path):\n",
    "    try:\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            features = resnet(img_tensor)\n",
    "        return features.cpu().squeeze().numpy()\n",
    "    except Exception as e:\n",
    "        # print(f\"Ошибка при обработке {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Предобработка текста\n",
    "# ----------------------------\n",
    "text_model = SentenceTransformer('cointegrated/rubert-tiny2') # или для множества языков: 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "text_model.to(device)\n",
    "\n",
    "def encode_texts(texts):\n",
    "    return text_model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Извлечение признаков\n",
    "# ----------------------------\n",
    "def build_features(df, img_dir):\n",
    "    img_paths = [os.path.join(img_dir, fname.split(\":\")[1]) for fname in df['filename']]\n",
    "    img_feats = []\n",
    "    for path in img_paths:\n",
    "        feat = extract_img_features(path)\n",
    "        if feat is None:\n",
    "            feat = [0] * 2048  # заглушка, если изображение не загружено\n",
    "        img_feats.append(feat)\n",
    "    text_feats = encode_texts(df['text'].tolist())\n",
    "    # Объединяем признаки\n",
    "    import numpy as np\n",
    "    combined = np.concatenate([img_feats, text_feats], axis=1)\n",
    "    return combined\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# def build_features(df, img_dir):\n",
    "#     img_paths = [os.path.join(img_dir, fname.split(\":\")[1]) for fname in df['filename']]\n",
    "#     img_feats = []\n",
    "#     for path in tqdm(img_paths, desc=\"Извлечение признаков изображений\"):\n",
    "#         feat = extract_img_features(path)\n",
    "#         if feat is None:\n",
    "#             feat = [0] * 2048  # заглушка\n",
    "#         img_feats.append(feat)\n",
    "#     text_feats = encode_texts(df['text'].tolist())\n",
    "#     import numpy as np\n",
    "#     combined = np.concatenate([img_feats, text_feats], axis=1)\n",
    "#     return combined\n",
    "\n",
    "print(\"Извлечение признаков для train...\")\n",
    "X_train = build_features(train_df, TRAIN_IMG_DIR)\n",
    "y_train = train_df['mark'].values\n",
    "\n",
    "print(\"Извлечение признаков для test...\")\n",
    "X_test = build_features(test_df, TEST_IMG_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Обучение модели\n",
    "# ----------------------------\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(X_train, y_train_enc)\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Предсказание\n",
    "# ----------------------------\n",
    "y_pred_enc = clf.predict(X_test)\n",
    "y_pred = le.inverse_transform(y_pred_enc)\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Сохранение результата\n",
    "# ----------------------------\n",
    "result = pd.DataFrame({\n",
    "    'filename': test_df['filename'],\n",
    "    'text': test_df['text'],\n",
    "    'mark': y_pred\n",
    "})\n",
    "result.to_csv(OUTPUT_FILE, index=False, sep='\\t')\n",
    "print(f\"Результат сохранён в {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a3cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Классификация изображений\n",
    "\n",
    "# 0.9692\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "# from torchvision.datasets import CIFAR10\n",
    "# # train_dataset = CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "# # val_dataset = CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers.modeling_outputs import ImageClassifierOutput\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# 1. Подготовка данных (Когда каждый класс находится в одной папке)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "train_dataset = ImageFolder(root=\"cifar10/train\", transform=transform)\n",
    "val_dataset = ImageFolder(root=\"cifar10/test\", transform=transform)\n",
    "\n",
    "\n",
    "\n",
    "# 1. Подготовка данных (Когда все в одной папке + file.csv)\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from PIL import Image\n",
    "# from torch.utils.data import Dataset\n",
    "\n",
    "# class CSVDataset(Dataset):\n",
    "#     def __init__(self, csv_file, img_root, transform=None):\n",
    "#         self.df = pd.read_csv(csv_file)\n",
    "#         self.img_root = img_root\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.df.iloc[idx]\n",
    "#         img_path = os.path.join(self.img_root, row['filename'])\n",
    "#         image = Image.open(img_path).convert(\"RGB\")\n",
    "#         label = int(row['label'])\n",
    "\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "#         return image, label\n",
    "\n",
    "# # Использование:\n",
    "# train_dataset = CSVDataset(\n",
    "#     csv_file=\"train_labels.csv\",\n",
    "#     img_root=\"images/\",\n",
    "#     transform=transform\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# 2. Модель\n",
    "model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "class WrappedResNet50(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, pixel_values=None, labels=None, **kwargs):\n",
    "        logits = self.model(pixel_values)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "        return ImageClassifierOutput(loss=loss, logits=logits)\n",
    "\n",
    "model_wrapped = WrappedResNet50(model)\n",
    "\n",
    "# 3. Collate function\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[0] for example in examples])\n",
    "    labels = torch.tensor([example[1] for example in examples], dtype=torch.long)\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "# 4. Метрика — обычный способ\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "# 5. Аргументы и Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./cifar10-resnet50\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    # save_strategy=\"epoch\",\n",
    "    logging_steps=1000,\n",
    "    num_train_epochs=3,\n",
    "    # fp16=torch.cuda.is_available(),\n",
    "    save_total_limit=2,\n",
    "    # load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"accuracy\",\n",
    "    # greater_is_better=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_wrapped,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Оценка модели\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Трансформации — точно такие же, как при обучении!\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Загружаем тестовые данные из папки \"test\"\n",
    "test_dataset = ImageFolder(root=\"cifar10/test\", transform=test_transform)\n",
    "\n",
    "# Убедимся, что порядок классов совпадает с CIFAR-10\n",
    "# CIFAR-10 классы в порядке: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "test_dataset.class_to_idx = {cls: idx for idx, cls in enumerate(cifar10_classes)}\n",
    "\n",
    "# DataLoader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# # Accuracy\n",
    "# model_wrapped.eval()\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         pixel_values = batch[0].to(model_wrapped.model.fc.weight.device)\n",
    "#         labels = batch[1]\n",
    "#         outputs = model_wrapped(pixel_values=pixel_values)\n",
    "#         preds = outputs.logits.argmax(dim=-1).cpu()\n",
    "#         all_preds.append(preds)\n",
    "#         all_labels.append(labels)\n",
    "\n",
    "# all_preds = torch.cat(all_preds)\n",
    "# all_labels = torch.cat(all_labels)\n",
    "\n",
    "# accuracy = (all_preds == all_labels).float().mean().item()\n",
    "# print(f\"Accuracy на папке test: {accuracy:.4f}\")\n",
    "\n",
    "# from pathlib import Path\n",
    "# import pandas as pd\n",
    "\n",
    "# filenames = [Path(p).name for p, _ in test_dataset.imgs]\n",
    "# # filenames = [Path(p).relative_to(\"cifar10/test\") for p, _ in test_dataset.imgs]\n",
    "\n",
    "# df = pd.DataFrame({\"filename\": filenames, \"label\": all_preds})\n",
    "# df.to_csv(\"predictions.csv\", index=False)\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "model_wrapped.eval()\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        pixel_values = batch[0].to(model_wrapped.model.fc.weight.device)\n",
    "        outputs = model_wrapped(pixel_values=pixel_values)\n",
    "        preds = outputs.logits.argmax(dim=-1).cpu()\n",
    "        all_preds.append(preds)\n",
    "\n",
    "all_preds = torch.cat(all_preds)\n",
    "\n",
    "# Получаем имена файлов (можно использовать .name или относительный путь)\n",
    "filenames = [Path(p).name for p, _ in test_dataset.imgs]\n",
    "\n",
    "# Сохраняем в CSV\n",
    "df = pd.DataFrame({\"filename\": filenames, \"label\": all_preds.tolist()})\n",
    "df.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG система\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "name_model = \"Qwen/Qwen3-4B\"\n",
    "name_encoder = 'all-MiniLM-L6-v2'\n",
    "\n",
    "print(\"Загружаем модели...\")\n",
    "encoder = SentenceTransformer(name_encoder)\n",
    "tokenizer = AutoTokenizer.from_pretrained(name_model, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name_model,\n",
    "    torch_dtype=\"auto\",#torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# === 2. База знаний ===\n",
    "documents = []\n",
    "with open('facts.txt', 'r', encoding='utf-8') as file:\n",
    "    documents = file.readlines()\n",
    "\n",
    "\n",
    "# === 3. FAISS индекс ===\n",
    "doc_embeddings = encoder.encode(documents, convert_to_numpy=True)\n",
    "index = faiss.IndexFlatL2(doc_embeddings.shape[1])\n",
    "index.add(doc_embeddings)\n",
    "\n",
    "# === 4. Основная функция ===\n",
    "def rag_answer_clean(query: str, top_k: int = 3, sim_threshold: float = 0.4) -> str:\n",
    "    start = time.time()\n",
    "    # Поиск\n",
    "    query_emb = encoder.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_emb, top_k)\n",
    "    # Преобразуем L2-дистанцию в схожесть (меньше дистанция — выше схожесть)\n",
    "    min_dist = distances[0][0]\n",
    "    similarity = 1 / (1 + min_dist)\n",
    "    if similarity < sim_threshold:\n",
    "        return \"Информация по запросу не найдена.\"\n",
    "    retrieved_docs = [documents[i] for i in indices[0]]\n",
    "    context = \"\\n\".join(retrieved_docs)\n",
    "    # Формируем ЧЁТКИЙ промпт\n",
    "    prompt = f\"\"\"Ответь кратко.\n",
    "Контекст:\n",
    "{context}\n",
    "Вопрос: {query}\n",
    "Ответ:\"\"\"\n",
    "    # Генерация\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():  # ускоряет и экономит память\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,      # только сколько нужно\n",
    "            do_sample=False,        # детерминировано = быстрее и стабильнее\n",
    "            temperature=0.0,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Извлекаем только часть после \"Ответ:\"\n",
    "    if \"Ответ:\" in full_output:\n",
    "        answer = full_output.split(\"Ответ:\", 1)[1].strip()\n",
    "    else:\n",
    "        answer = full_output[len(prompt):].strip()\n",
    "    # Убираем возможный \"Вопрос:\" в ответе\n",
    "    if \"\\nВопрос:\" in answer:\n",
    "        answer = answer.split(\"\\nВопрос:\")[0].strip()\n",
    "    print(f\"Время: {time.time() - start:.2f} сек\")\n",
    "    return answer\n",
    "\n",
    "q = \"FX_company какие занимает места?\"#\"Капитализация компании Яндекс\"\n",
    "otvet = rag_answer_clean(q)\n",
    "print(\"Вопрос:\", q)\n",
    "print(\"Ответ:\", otvet.split(\"\\n\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082966be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание целевой метки с помощью признаков и картинок:\n",
    "# 0.2848454177769526 - 0.778\n",
    "\n",
    "# 0.2848454177769526 - 0.778\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Загрузка и подготовка числовых признаков\n",
    "# -----------------------------\n",
    "# Числовые признаки (НЕ категориальные!)\n",
    "NUM_FEATURES = ['mileage', 'latitude', 'longitude', 'crashes_count', 'doors_number']\n",
    "\n",
    "# Категориальные признаки (оставляем только действительно категориальные)\n",
    "CAT_FEATURES = ['equipment', 'body_type', 'drive_type', 'engine_type', 'color', 'pts',\n",
    "                'audiosistema', 'diski', 'electropodemniki', 'fary', 'salon',\n",
    "                'upravlenie_klimatom', 'usilitel_rul', 'steering_wheel', 'owners_count']\n",
    "\n",
    "# Мультизначные признаки — обработаем отдельно\n",
    "MULTI_FEATURES = [\n",
    "    'aktivnaya_bezopasnost_mult', 'audiosistema_mult', 'shini_i_diski_mult',\n",
    "    'electroprivod_mult', 'fary_mult', 'multimedia_navigacia_mult',\n",
    "    'obogrev_mult', 'pamyat_nastroek_mult', 'podushki_bezopasnosti_mult',\n",
    "    'pomosh_pri_vozhdenii_mult', 'protivoygonnaya_sistema_mult', 'salon_mult',\n",
    "    'upravlenie_klimatom_mult'\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Обработка числовых признаков\n",
    "# -----------------------------\n",
    "def preprocess_numeric(df):\n",
    "    df = df.copy()\n",
    "    # crashes_count и doors_number — изначально числа, но могут быть строками\n",
    "    for col in ['crashes_count', 'doors_number']:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "    \n",
    "    # owners_count: \"> 3\" → 4 или 999\n",
    "    df['owners_count'] = df['owners_count'].replace({'> 3': '4'}).astype(str)\n",
    "    \n",
    "    # Числовые признаки — оставляем как числа!\n",
    "    for col in NUM_FEATURES:\n",
    "        if col != 'owners_count':  # он будет категориальным\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(df[col].median())\n",
    "    return df\n",
    "\n",
    "train_df = pd.read_parquet(\"train_dataset.parquet\")#[0:7000]\n",
    "test_df = pd.read_parquet(\"test_dataset.parquet\")#[0:2500]\n",
    "\n",
    "# Добавляем путь к изображениям\n",
    "train_df[\"img_path\"] = train_df[\"ID\"].apply(lambda x: f\"train_images/{x}_0.jpg\")\n",
    "test_df[\"img_path\"] = test_df[\"ID\"].apply(lambda x: f\"test_images/{x}_0.jpg\")\n",
    "\n",
    "train_df = preprocess_numeric(train_df)\n",
    "test_df = preprocess_numeric(test_df)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Обработка мультипризнаков: создадим бинарные флаги для частых значений\n",
    "# -----------------------------\n",
    "def extract_top_multilabels(train_df, test_df, multi_cols, top_n=10):\n",
    "    all_features = {}\n",
    "    for col in multi_cols:\n",
    "        # Собираем все уникальные ненулевые значения\n",
    "        values = []\n",
    "        for val in train_df[col]:\n",
    "            if isinstance(val, list):\n",
    "                values.extend([v for v in val if v is not None and v != 'None'])\n",
    "        # Топ-N самых частых\n",
    "        top_vals = pd.Series(values).value_counts().head(top_n).index.tolist()\n",
    "        all_features[col] = top_vals\n",
    "        \n",
    "        # Создаем бинарные признаки\n",
    "        for feat in top_vals:\n",
    "            train_df[f\"{col}_{feat}\"] = train_df[col].apply(\n",
    "                lambda x: 1 if isinstance(x, list) and feat in x else 0\n",
    "            )\n",
    "            test_df[f\"{col}_{feat}\"] = test_df[col].apply(\n",
    "                lambda x: 1 if isinstance(x, list) and feat in x else 0\n",
    "            )\n",
    "    \n",
    "    # Удаляем исходные мульти-столбцы\n",
    "    train_df = train_df.drop(columns=multi_cols)\n",
    "    test_df = test_df.drop(columns=multi_cols)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = extract_top_multilabels(train_df, test_df, MULTI_FEATURES, top_n=8)\n",
    "\n",
    "# Обновим список категориальных признаков (новые мульти-флаги — числовые!)\n",
    "# А owners_count теперь тоже категориальный (оставляем как строку)\n",
    "CAT_FEATURES = [col for col in CAT_FEATURES if col != 'owners_count'] + ['owners_count']\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Обработка категориальных признаков: замена None на \"MISSING\"\n",
    "# -----------------------------\n",
    "def fill_cat_missing(df, cat_cols):\n",
    "    df = df.copy()\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].fillna(\"MISSING\").astype(str)\n",
    "        # Заменяем 'None' (как строка) тоже на \"MISSING\"\n",
    "        df[col] = df[col].replace({\"None\": \"MISSING\", \"nan\": \"MISSING\"})\n",
    "    return df\n",
    "\n",
    "train_df = fill_cat_missing(train_df, CAT_FEATURES)\n",
    "test_df = fill_cat_missing(test_df, CAT_FEATURES)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Загрузка эмбеддингов (без изменений)\n",
    "# -----------------------------\n",
    "# EMBEDDINGS_DIR = \"embeddings\"\n",
    "# train_emb_path = os.path.join(EMBEDDINGS_DIR, \"train_embeddings.npy\")\n",
    "# test_emb_path = os.path.join(EMBEDDINGS_DIR, \"test_embeddings.npy\")\n",
    "\n",
    "# train_features = np.load(train_emb_path)\n",
    "# test_features = np.load(test_emb_path)\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Загружаем ResNet18 без последнего полносвязного слоя\n",
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])  # удаляем последний слой (1000-классовый)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Трансформации (как при обучении ImageNet)\n",
    "transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def extract_features(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model(img)\n",
    "    return features.squeeze().cpu().numpy()  # (512,)\n",
    "\n",
    "# # Для train\n",
    "# print(\"Extracting train features...\")\n",
    "# train_features = np.array([extract_features(p) for p in train_df[\"img_path\"]])\n",
    "\n",
    "# # Для test\n",
    "# print(\"Extracting test features...\")\n",
    "# test_features = np.array([extract_features(p) for p in test_df[\"img_path\"]])\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "EMBEDDINGS_DIR = \"embeddings\"\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "\n",
    "train_emb_path = os.path.join(EMBEDDINGS_DIR, \"train_embeddings.npy\")\n",
    "test_emb_path = os.path.join(EMBEDDINGS_DIR, \"test_embeddings.npy\")\n",
    "\n",
    "# --- Train embeddings ---\n",
    "if os.path.exists(train_emb_path):\n",
    "    print(\"Loading cached train embeddings...\")\n",
    "    train_features = np.load(train_emb_path)\n",
    "else:\n",
    "    print(\"Extracting train features...\")\n",
    "    train_features = np.array([extract_features(p) for p in tqdm(train_df[\"img_path\"])])\n",
    "    np.save(train_emb_path, train_features)\n",
    "    print(f\"Train embeddings saved to {train_emb_path}\")\n",
    "\n",
    "# --- Test embeddings ---\n",
    "if os.path.exists(test_emb_path):\n",
    "    print(\"Loading cached test embeddings...\")\n",
    "    test_features = np.load(test_emb_path)\n",
    "else:\n",
    "    print(\"Extracting test features...\")\n",
    "    test_features = np.array([extract_features(p) for p in tqdm(test_df[\"img_path\"])])\n",
    "    np.save(test_emb_path, test_features)\n",
    "    print(f\"Test embeddings saved to {test_emb_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Сборка финального датасета\n",
    "# -----------------------------\n",
    "# Числовые эмбеддинги\n",
    "num_feature_names = [f'img_feat_{i}' for i in range(train_features.shape[1])]\n",
    "train_num_df = pd.DataFrame(train_features, columns=num_feature_names)\n",
    "test_num_df = pd.DataFrame(test_features, columns=num_feature_names)\n",
    "\n",
    "# Числовые признаки из данных\n",
    "train_num_meta = train_df[NUM_FEATURES].copy()\n",
    "test_num_meta = test_df[NUM_FEATURES].copy()\n",
    "\n",
    "# Категориальные признаки\n",
    "train_cat = train_df[CAT_FEATURES].copy()\n",
    "test_cat = test_df[CAT_FEATURES].copy()\n",
    "\n",
    "# Все фичи: эмбеддинги + числовые мета + категориальные + мульти-флаги (уже в df)\n",
    "X_train = pd.concat([train_num_df, train_num_meta, train_cat], axis=1)\n",
    "X_test = pd.concat([test_num_df, test_num_meta, test_cat], axis=1)\n",
    "\n",
    "# Проверим, что все числовые признаки — числа, а категориальные — строки\n",
    "assert X_train[NUM_FEATURES].dtypes.apply(lambda x: np.issubdtype(x, np.number)).all()\n",
    "assert X_train[CAT_FEATURES].dtypes.apply(lambda x: x == 'object').all()\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Обучение CatBoost с валидацией\n",
    "# -----------------------------\n",
    "y_train = train_df[\"price_TARGET\"].values\n",
    "\n",
    "# Разделим train на train/val для ранней остановки\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=2000,\n",
    "    learning_rate=0.03,\n",
    "    depth=6,\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    cat_features=CAT_FEATURES,\n",
    "    early_stopping_rounds=100,\n",
    "    verbose=100,\n",
    "    random_seed=42,\n",
    "    use_best_model=True\n",
    ")\n",
    "\n",
    "cat_model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    eval_set=(X_val, y_val),\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Предсказание и сохранение\n",
    "# -----------------------------\n",
    "preds = cat_model.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df[\"ID\"],\n",
    "    \"target\": preds\n",
    "})\n",
    "submission.to_csv(\"submissionV17_mape.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d94f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание целевой метки с помощью признаков и картинок (худшая версия):\n",
    "# 0.307417972160942 - 0.765\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "train_df = pd.read_parquet(\"train_dataset.parquet\")#[0:7000]\n",
    "test_df = pd.read_parquet(\"test_dataset.parquet\")#[0:2500]\n",
    "\n",
    "# Добавляем путь к изображениям\n",
    "train_df[\"img_path\"] = train_df[\"ID\"].apply(lambda x: f\"train_images/{x}_0.jpg\")\n",
    "test_df[\"img_path\"] = test_df[\"ID\"].apply(lambda x: f\"test_images/{x}_0.jpg\")\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Загружаем ResNet18 без последнего полносвязного слоя\n",
    "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])  # удаляем последний слой (1000-классовый)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Трансформации (как при обучении ImageNet)\n",
    "transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def extract_features(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img = transform(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        features = model(img)\n",
    "    return features.squeeze().cpu().numpy()  # (512,)\n",
    "# # Для train\n",
    "# print(\"Extracting train features...\")\n",
    "# train_features = np.array([extract_features(p) for p in train_df[\"img_path\"]])\n",
    "\n",
    "# # Для test\n",
    "# print(\"Extracting test features...\")\n",
    "# test_features = np.array([extract_features(p) for p in test_df[\"img_path\"]])\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "EMBEDDINGS_DIR = \"embeddings\"\n",
    "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "\n",
    "train_emb_path = os.path.join(EMBEDDINGS_DIR, \"train_embeddings.npy\")\n",
    "test_emb_path = os.path.join(EMBEDDINGS_DIR, \"test_embeddings.npy\")\n",
    "\n",
    "# --- Train embeddings ---\n",
    "if os.path.exists(train_emb_path):\n",
    "    print(\"Loading cached train embeddings...\")\n",
    "    train_features = np.load(train_emb_path)\n",
    "else:\n",
    "    print(\"Extracting train features...\")\n",
    "    train_features = np.array([extract_features(p) for p in tqdm(train_df[\"img_path\"])])\n",
    "    np.save(train_emb_path, train_features)\n",
    "    print(f\"Train embeddings saved to {train_emb_path}\")\n",
    "\n",
    "# --- Test embeddings ---\n",
    "if os.path.exists(test_emb_path):\n",
    "    print(\"Loading cached test embeddings...\")\n",
    "    test_features = np.load(test_emb_path)\n",
    "else:\n",
    "    print(\"Extracting test features...\")\n",
    "    test_features = np.array([extract_features(p) for p in tqdm(test_df[\"img_path\"])])\n",
    "    np.save(test_emb_path, test_features)\n",
    "    print(f\"Test embeddings saved to {test_emb_path}\")\n",
    "meta_features = ['equipment', 'body_type', 'drive_type', 'engine_type',\n",
    "       'doors_number', 'color', 'pts', 'audiosistema', 'diski',\n",
    "       'electropodemniki', 'fary', 'salon', 'upravlenie_klimatom',\n",
    "       'usilitel_rul', 'steering_wheel', 'crashes_count', 'owners_count',\n",
    "       'mileage', 'latitude', 'longitude', 'aktivnaya_bezopasnost_mult',\n",
    "       'audiosistema_mult', 'shini_i_diski_mult', 'electroprivod_mult',\n",
    "       'fary_mult', 'multimedia_navigacia_mult', 'obogrev_mult',\n",
    "       'pamyat_nastroek_mult', 'podushki_bezopasnosti_mult',\n",
    "       'pomosh_pri_vozhdenii_mult', 'protivoygonnaya_sistema_mult',\n",
    "       'salon_mult', 'upravlenie_klimatom_mult']\n",
    "train_features\n",
    "train_df[meta_features].head(3)\n",
    "# X_train = train_features\n",
    "y_train = train_df[\"price_TARGET\"].values\n",
    "\n",
    "for col in tqdm(meta_features):\n",
    "    # if train_df[col].dtype == 'object':\n",
    "    train_df[col] = train_df[col].fillna(\"0\").astype(str)\n",
    "    test_df[col] = test_df[col].fillna(\"0\").astype(str) \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Преобразуем численные признаки в DataFrame с именами столбцов\n",
    "num_feature_names = [f'feat_{i}' for i in range(train_features.shape[1])]\n",
    "train_num_df = pd.DataFrame(train_features, columns=num_feature_names)\n",
    "\n",
    "# 2. Обработаем мета-признаки (списки → строки)\n",
    "meta_df = train_df[meta_features].copy()\n",
    "for col in meta_df.columns:\n",
    "    meta_df[col] = meta_df[col].apply(lambda x: ', '.join(str(i) for i in x) if isinstance(x, list) else str(x) if pd.notna(x) else '')\n",
    "\n",
    "# 3. Объединим\n",
    "X_train = pd.concat([train_num_df, meta_df], axis=1)\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Обучаем регрессор\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=1000,\n",
    "    # learning_rate=0.01,\n",
    "    # depth=6,\n",
    "    loss_function='MAE',\n",
    "    verbose=100,\n",
    "    random_seed=42,\n",
    "    cat_features=meta_features,\n",
    "    eval_metric='MAE'\n",
    "    \n",
    ")\n",
    "\n",
    "cat_model.fit(X_train, y_train)\n",
    "# 1. Преобразуем численные признаки в DataFrame с именами столбцов\n",
    "num_feature_names = [f'feat_{i}' for i in range(test_features.shape[1])]\n",
    "test_num_df = pd.DataFrame(test_features, columns=num_feature_names)\n",
    "\n",
    "# 2. Обработаем мета-признаки (списки → строки)\n",
    "meta_df = test_df[meta_features].copy()\n",
    "for col in meta_df.columns:\n",
    "    meta_df[col] = meta_df[col].apply(lambda x: ', '.join(str(i) for i in x) if isinstance(x, list) else str(x) if pd.notna(x) else '')\n",
    "\n",
    "# 3. Объединим\n",
    "X_test = pd.concat([test_num_df, meta_df], axis=1)\n",
    "preds = cat_model.predict(X_test)\n",
    "\n",
    "# Формируем submission\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df[\"ID\"],\n",
    "    \"target\": preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"submissionV15.csv\", index=False)\n",
    "print(\"✅ Submission saved to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
